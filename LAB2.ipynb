{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "L8TCN6eqP80G",
        "nxMs9GuSRfg1",
        "VqsR3Sl_RioR",
        "HAJNv8MWR22j"
      ],
      "authorship_tag": "ABX9TyOrzyzD3ZZ+ldiKFvqbiIDu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VulgarBOMB/LAB2/blob/main/LAB2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpJcju8mwudg"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!mkdir task1\n",
        "!mkdir task2\n",
        "!mkdir task3\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, avg, min, max, to_date, floor, coalesce, lit, months_between\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\") \\\n",
        "                    .appName('sparkProject') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "jobsDF = spark.read.parquet(\"jobs.parquet\")\n",
        "employeesDF = spark.read.parquet(\"employees.parquet\")\n",
        "departmentsDF = spark.read.parquet(\"department.parquet\")\n",
        "locationsDF = spark.read.parquet(\"locations.parquet\")\n",
        "regionsDF = spark.read.parquet(\"regions.parquet\")\n",
        "countriesDF = spark.read.parquet(\"countries.parquet\")\n",
        "job_historyDF = spark.read.parquet(\"job_history.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проверка данных в паркетниках\n",
        "Реализация проверок на типы данных, дубликаты и Null-значения"
      ],
      "metadata": {
        "id": "L8TCN6eqP80G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def checkDataTypes(jobsDF, locationsDF, departmentsDF, employeesDF):\n",
        "  if dict(locationsDF.dtypes)['location_id'] != 'int':\n",
        "    print('Типы данных в LOCATIONS некорректные')\n",
        "    return False\n",
        "  elif dict(departmentsDF.dtypes)['department_id'] != 'int':\n",
        "    print('Типы данных в DEPARTMENTS некорректные')\n",
        "    return False\n",
        "  elif dict(employeesDF.dtypes)['salary'] != 'float'\\\n",
        "    or dict(employeesDF.dtypes)['department_id'] != 'int':\n",
        "    print('Типы данных в EMPLOYEES некорректные')\n",
        "    return False\n",
        "  elif dict(jobsDF.dtypes)['min_salary'] != 'float':\n",
        "    print('Типы данных в JOBS некорректные')\n",
        "    return False\n",
        "  else:\n",
        "    print('Все типы данных корректные')\n",
        "    return True\n",
        "\n",
        "def checkWholeNullItem(jobsDF, locationsDF, departmentsDF, employeesDF):\n",
        "  if locationsDF.filter(\n",
        "      col('location_id').isNull() |\n",
        "      col('location_id').isNull()\n",
        "  ).count() > 0:\n",
        "    print('JOBS имеет NULL-значения:')\n",
        "    locationsDF.filter((col('location_id').isNull()) |\n",
        "                      (col('state_province').isNull())).show()\n",
        "    return False\n",
        "  elif departmentsDF.filter(\n",
        "      col('department_id').isNull() |\n",
        "      col('department_name').isNull()\n",
        "  ).count() > 0:\n",
        "    print('DEPARTMENTS имеет NULL-значения:')\n",
        "    departmentsDF.filter((col('department_id').isNull()) |\n",
        "                         (col('department_name').isNull())).show()\n",
        "    return False\n",
        "  elif employeesDF.filter(\n",
        "      col('first_name').isNull() |\n",
        "      col('last_name').isNull() |\n",
        "      col('job_id').isNull() |\n",
        "      col('salary').isNull() |\n",
        "      col('commission_pct').isNull() |\n",
        "      col('hire_date').isNull() |\n",
        "      col('department_id').isNull()\n",
        "  ).count() > 0:\n",
        "    print('EMPLOYEES имеет NULL-значения:')\n",
        "    employeesDF.filter((col('first_name').isNull()) |\n",
        "                       (col('last_name').isNull()) |\n",
        "                       (col('job_id').isNull()) |\n",
        "                       (col('salary').isNull()) |\n",
        "                       (col('commission_pct').isNull()) |\n",
        "                       (col('hire_date').isNull()) |\n",
        "                       (col('department_id').isNull())).show(10000)\n",
        "    return False\n",
        "  elif jobsDF.filter(\n",
        "      col('job_id').isNull() |\n",
        "      col('job_title').isNull() |\n",
        "      col('min_salary').isNull()\n",
        "  ).count() > 0:\n",
        "    print('JOBS имеет NULL-значения:')\n",
        "    jobsDF.filter((col('job_id').isNull()) |\n",
        "                  (col('job_title').isNull()) |\n",
        "                  (col('min_salary').isNull())).show()\n",
        "    return False\n",
        "  else:\n",
        "    print('Нет NULL-значений в таблицах')\n",
        "    return True\n",
        "\n",
        "def checkDuplicateRows(jobsDF, locationsDF, departmentsDF, employeesDF):\n",
        "  department_counts = departmentsDF\\\n",
        "    .groupBy(\"department_name\")\\\n",
        "    .agg(count(\"*\")\\\n",
        "    .alias(\"count\"))\\\n",
        "    .filter(col(\"count\") > 1)\\\n",
        "    .count()\n",
        "\n",
        "  location_counts = locationsDF\\\n",
        "    .groupBy(\"location_id\", \"state_province\")\\\n",
        "    .agg(count(\"*\")\\\n",
        "    .alias(\"count\"))\\\n",
        "    .filter(col(\"count\") > 1)\\\n",
        "    .count()\n",
        "\n",
        "  employee_counts = employeesDF\\\n",
        "    .groupBy(\"first_name\", \"last_name\", \"job_id\", \"salary\", \"department_id\")\\\n",
        "    .agg(count(\"*\")\\\n",
        "    .alias(\"count\"))\\\n",
        "    .filter(col(\"count\") > 1)\\\n",
        "    .count()\n",
        "\n",
        "  job_counts = jobsDF\\\n",
        "    .groupBy(\"job_title\")\\\n",
        "    .agg(count(\"*\")\\\n",
        "    .alias(\"count\"))\\\n",
        "    .filter(col(\"count\") > 1)\\\n",
        "    .count()\n",
        "\n",
        "  if (department_counts + location_counts + employee_counts + job_counts) != 0:\n",
        "    print('В одной из таблиц найдены дубликаты')\n",
        "    return False\n",
        "  else:\n",
        "    print('Дубликаты не были найдены')\n",
        "    return True"
      ],
      "metadata": {
        "id": "afQyqoMWxRF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 1"
      ],
      "metadata": {
        "id": "nxMs9GuSRfg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "departmentsDF.createOrReplaceTempView('departments')\n",
        "employeesDF.createOrReplaceTempView('employees')\n",
        "jobsDF.createOrReplaceTempView('jobs')\n",
        "\n",
        "# spark.sql(\"\"\"\n",
        "# SELECT d.department_name,\n",
        "#       e.employee_id,\n",
        "#       e.first_name,\n",
        "#       e.last_name,\n",
        "#       e.salary,\n",
        "#       j.max_salary,\n",
        "#       (SELECT AVG(salary) FROM employees WHERE department_id = d.department_id) AS avg_salary\n",
        "# FROM employees e\n",
        "# LEFT JOIN departments d ON d.department_id=e.department_id\n",
        "# LEFT JOIN jobs j ON e.job_id = j.job_id\n",
        "# WHERE e.salary > (\n",
        "#   SELECT AVG(salary)\n",
        "#   FROM employees\n",
        "#   WHERE department_id = d.department_id\n",
        "# )\n",
        "# ORDER BY d.department_name\n",
        "# \"\"\").show()\n",
        "\n",
        "if (checkDataTypes(jobsDF, locationsDF, departmentsDF, employeesDF) &\n",
        "checkDuplicateRows(jobsDF, locationsDF, departmentsDF, employeesDF)):\n",
        "\n",
        "  checkWholeNullItem(jobsDF, locationsDF, departmentsDF, employeesDF)\n",
        "\n",
        "  joiningDF = employeesDF.join(departmentsDF, departmentsDF.department_id == employeesDF.department_id, \"left\").\\\n",
        "                          join(jobsDF, jobsDF.job_id == employeesDF.job_id, \"left\").\\\n",
        "                          select(departmentsDF.department_name, \\\n",
        "                                 employeesDF.employee_id, \\\n",
        "                                 employeesDF.first_name, \\\n",
        "                                 employeesDF.last_name, \\\n",
        "                                 employeesDF.salary, \\\n",
        "                                 jobsDF.max_salary, \\\n",
        "                                 employeesDF.department_id)\n",
        "  avg_salaryDF = employeesDF.groupBy(\"department_id\").\\\n",
        "                             agg(avg(\"salary\").alias(\"avg_salary\"))\n",
        "  resultDF = joiningDF.join(avg_salaryDF, 'department_id', \"left\").\\\n",
        "                       drop('department_id').\\\n",
        "                       filter(joiningDF.salary > avg_salaryDF.avg_salary).\\\n",
        "                       orderBy(joiningDF.department_name)\n",
        "\n",
        "  resultDF.write.mode('overwrite').parquet(\"/content/task1\")\n",
        "  resultDF.show()\n",
        "\n",
        "else:\n",
        "  print('Не пройдено одно из условий выше ↑')"
      ],
      "metadata": {
        "id": "arPMXPNMxVs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 2"
      ],
      "metadata": {
        "id": "VqsR3Sl_RioR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employeesDF.createOrReplaceTempView(\"employees\")\n",
        "\n",
        "if (checkDataTypes(jobsDF, locationsDF, departmentsDF, employeesDF) &\n",
        "checkDuplicateRows(jobsDF, locationsDF, departmentsDF, employeesDF)):\n",
        "\n",
        "  checkWholeNullItem(jobsDF, locationsDF, departmentsDF, employeesDF)\n",
        "\n",
        "  # Вариант с использованием DataFrame API\n",
        "  resultDF = employeesDF.groupBy(employeesDF.job_id).\\\n",
        "             agg(avg(employeesDF.salary))\n",
        "  resultDF.show()\n",
        "\n",
        "  # # Вариант с использованием нативного SQL\n",
        "  # resultDF = spark.sql(\"\"\"\n",
        "  # SELECT job_id, AVG(salary) FROM employees GROUP BY job_id;\n",
        "  # \"\"\")\n",
        "\n",
        "  # Преобразование в dictionary\n",
        "  pair_rdd = resultDF.rdd.map(lambda x: (x[0], x[1]))\n",
        "  # print(resultDF.rdd.collect())\n",
        "  dictionary = pair_rdd.collectAsMap()\n",
        "  print(dictionary)\n",
        "\n",
        "  resultDF.write.mode('overwrite').parquet(\"/content/task2\")\n",
        "\n",
        "else:\n",
        "  print('Не пройдено одно из условий выше ↑')"
      ],
      "metadata": {
        "id": "7GmujWQHxg3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 3"
      ],
      "metadata": {
        "id": "HAJNv8MWR22j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "employeesDF = employeesDF.withColumn('hire_date', to_date('hire_date', 'dd.mm.yyyy'))\n",
        "employeesDF.createOrReplaceTempView('employees')\n",
        "departmentsDF.createOrReplaceTempView('departments')\n",
        "locationsDF.createOrReplaceTempView('locations')\n",
        "\n",
        "# spark.sql(\"\"\"\n",
        "# SELECT l.state_province,\n",
        "#        e.employee_id,\n",
        "#        e.hire_date,\n",
        "#        floor(months_between('2017-10-01', e.hire_date)) as months,\n",
        "#        e.salary,\n",
        "#        COALESCE(e.commission_pct, 0) as commission_pct,\n",
        "#        e.salary * months as earned_money_before_vat,\n",
        "#        earned_money_before_vat - COALESCE((e.commission_pct * earned_money_before_vat), 0) as earned_money_after_vat\n",
        "# FROM employees e\n",
        "# LEFT JOIN departments d ON e.department_id = d.department_id\n",
        "# LEFT JOIN locations l ON d.location_id = l.location_id\n",
        "# WHERE e.hire_date <= '2017-10-01' AND e.salary*floor(months_between('2017-10-01', e.hire_date)) = (\n",
        "#   SELECT max(e.salary * floor(months_between('2017-10-01', e.hire_date)))\n",
        "#   FROM employees e\n",
        "#   WHERE e.hire_date <= '2017-10-01'\n",
        "# )\n",
        "# UNION ALL\n",
        "# SELECT l.state_province,\n",
        "#        e.employee_id,\n",
        "#        e.hire_date,\n",
        "#        floor(months_between('2017-10-01', e.hire_date)) as months,\n",
        "#        e.salary,\n",
        "#        COALESCE(e.commission_pct, 0) as commission_pct,\n",
        "#        e.salary * months as earned_money_before_vat,\n",
        "#        earned_money_before_vat - COALESCE((e.commission_pct * earned_money_before_vat), 0) as earned_money_after_vat\n",
        "# FROM employees e\n",
        "# LEFT JOIN departments d ON e.department_id = d.department_id\n",
        "# LEFT JOIN locations l ON d.location_id = l.location_id\n",
        "# WHERE e.hire_date <= '2017-10-01' AND e.salary*floor(months_between('2017-10-01', e.hire_date)) = (\n",
        "#   SELECT min(e.salary * floor(months_between('2017-10-01', e.hire_date)))\n",
        "#   FROM employees e\n",
        "#   WHERE e.hire_date <= '2017-10-01'\n",
        "# );\n",
        "# \"\"\").show(truncate=False)\n",
        "\n",
        "# spark.sql(\"\"\"\n",
        "#  WITH salary_extremes AS (\n",
        "#    SELECT\n",
        "#      MAX(e.salary * floor(months_between('2017-10-01', e.hire_date))) as max_salary,\n",
        "#     MIN(e.salary * floor(months_between('2017-10-01', e.hire_date))) as min_salary\n",
        "#    FROM employees e\n",
        "#    WHERE e.hire_date <= '2017-10-01'\n",
        "#  )\n",
        "\n",
        "#  SELECT l.state_province,\n",
        "#        e.employee_id,\n",
        "#        e.hire_date,\n",
        "#        floor(months_between('2017-10-01', e.hire_date)) as months,\n",
        "#        e.salary,\n",
        "#        COALESCE(e.commission_pct, 0) as commission_pct,\n",
        "#        e.salary * months as earned_money_before_vat,\n",
        "#        earned_money_before_vat - COALESCE((e.commission_pct * earned_money_before_vat), 0) as earned_money_after_vat\n",
        "#  FROM employees e\n",
        "#  LEFT JOIN departments d ON e.department_id = d.department_id\n",
        "#  LEFT JOIN locations l ON d.location_id = l.location_id,\n",
        "#     salary_extremes se\n",
        "#  WHERE e.hire_date <= '2017-10-01'\n",
        "#  AND (e.salary*floor(months_between('2017-10-01', e.hire_date)) = se.max_salary\n",
        "#  OR e.salary*floor(months_between('2017-10-01', e.hire_date)) = se.min_salary);\n",
        "#  \"\"\").show(truncate=False)\n",
        "\n",
        "\n",
        "if (checkDataTypes(jobsDF, locationsDF, departmentsDF, employeesDF) &\n",
        "  checkDuplicateRows(jobsDF, locationsDF, departmentsDF, employeesDF)):\n",
        "\n",
        "  checkWholeNullItem(jobsDF, locationsDF, departmentsDF, employeesDF)\n",
        "\n",
        "  # Соединение dataframe\n",
        "  df = employeesDF.alias('e')\\\n",
        "      .join(departmentsDF.alias('d'), col('e.department_id') == col('d.department_id'), 'left')\\\n",
        "      .join(locationsDF.alias('l'), col('d.location_id') == col('l.location_id'), 'left')\n",
        "\n",
        "  # Создание калькулируемых столбцов\n",
        "  df = df.withColumn('months', floor(months_between(lit('2017-10-01'), col('e.hire_date'))))\\\n",
        "      .withColumn('commission_pct', coalesce(col('e.commission_pct'), lit(0)))\\\n",
        "      .withColumn('earned_money_before_vat', col('e.salary') * col('months'))\\\n",
        "      .withColumn('earned_money_after_vat', col('earned_money_before_vat') - (col('commission_pct') * col('earned_money_before_vat')))\\\n",
        "      .select(col('l.state_province'),\\\n",
        "              col('e.employee_id'),\\\n",
        "              col('e.hire_date'),\\\n",
        "              col('months'),\\\n",
        "              col('e.salary'),\\\n",
        "              col('commission_pct'),\\\n",
        "              col('earned_money_before_vat'),\\\n",
        "              col('earned_money_after_vat'))\n",
        "\n",
        "  # Получение мин и макс ЗП\n",
        "  max_salary = df.filter(df.hire_date <= '2017-10-01')\\\n",
        "      .agg(max(df.salary * df.months)).collect()[0][0]\n",
        "  print(df.collect()[0].__class__)\n",
        "  min_salary = df.filter(df.hire_date <= '2017-10-01')\\\n",
        "      .agg(min(df.salary * df.months)).first()[0]\n",
        "\n",
        "  # Конечный фильтр\n",
        "  df = df.filter((df.hire_date <= '2017-10-01') & ((df.salary * df.months) == max_salary) | ((df.salary * df.months) == min_salary))\n",
        "  df.show()\n",
        "  df.write.mode('overwrite').parquet(\"/content/task3\")\n",
        "\n",
        "else:\n",
        "  print('Не пройдено одно из условий выше ↑')"
      ],
      "metadata": {
        "id": "szQEWSFgy3ig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}